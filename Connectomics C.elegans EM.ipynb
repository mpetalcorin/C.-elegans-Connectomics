{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af399b22-57d6-452c-9e04-b66997372661",
   "metadata": {},
   "source": [
    "# CPU proof-of-concept connectomics pipeline on a small public *C. elegans* EM cutout.\n",
    "Connectomes are maps of synaptic wiring that help explain how nervous systems produce behaviour. The *C. elegans* connectome showed that a complete wiring diagram is possible and biologically useful, and later reconstructions improved coverage, accuracy, and developmental, sex-specific insights. However, volume EM generates enormous image stacks, and turning them into neuron and synapse graphs requires robust image processing, machine learning, and long-term computing infrastructure. Modern pipelines use ML plus human proofreading platforms, but running the full workflow end to end is often out of reach on modest hardware. To bridge this gap, we built a CPU-friendly proof of concept using a small BossDB cutout, implementing each stage in a transparent, reproducible way, producing diagnostic figures and intermediate outputs that can be refined by humans and later upgraded to deep learning.\n",
    "## Data source: BossDB (downloads only a small cutout).\n",
    "## Default dataset URI: bossdb://mulcahy2022/1h_L1/em\n",
    "\n",
    "## Pipeline steps:\n",
    "1) Download cutout stack (Z,Y,X) as numpy\n",
    "2) Preprocess: denoise + normalize + CLAHE\n",
    "3) Align slices (rigid translation via phase correlation)\n",
    "4) Segment neurites (2D watershed per slice + simple 3D stitching)\n",
    "5) Detect synapse candidates (heuristic, blob-like dark features near inter-neurite boundaries)\n",
    "6) Build wiring graph (NetworkX) + basic stats\n",
    "7) Save publishable figures (PDF/PNG)\n",
    "\n",
    "## New flags for small machines:\n",
    "- --preset safe8gb, sets a conservative cutout\n",
    "- --save_minimal, saves only aligned volume, labels, tables, figures\n",
    "- --aligned_dtype float16, halves aligned volume disk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e831538d-dbb4-4373-9577-713116dff033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys, pkgutil\n",
    "print(sys.executable)\n",
    "print(\"intern\" in [m.name for m in pkgutil.iter_modules()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94c33d8-839b-486a-b82c-dc559ae71237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Planned run settings\n",
      "Dataset URI, bossdb://mulcahy2022/1h_L1/em\n",
      "Cutout (X,Y,Z), (256, 256, 64)\n",
      "Aligned dtype, float32\n",
      "save_minimal, False\n",
      "\n",
      "Approximate single-volume sizes\n",
      "Raw uint8 volume, 0.004 GB\n",
      "Preprocessed float32 volume, 0.016 GB\n",
      "Aligned float32 volume, 0.016 GB\n",
      "\n",
      "[1/8] Downloading cutout from: bossdb://mulcahy2022/1h_L1/em\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/intern/convenience/array.py:937: ImportWarning: CloudVolume is not installed. Accessing channel using CVDB.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/8] Preprocessing (denoise + CLAHE)\n",
      "[3/8] Alignment\n",
      "Simulating misalignment, then correcting it\n",
      "[4/8] Segmentation (watershed baseline + 3D stitching)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting slices: 100%|██████████| 64/64 [00:00<00:00, 70.71it/s]\n",
      "Stitching 3D IDs: 100%|██████████| 63/63 [00:00<00:00, 92.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/8] Synapse candidate detection (heuristic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting synapse candidates: 100%|██████████| 64/64 [00:05<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/8] Build wiring graph\n",
      "[7/8] Saving figures\n",
      "[8/8] Logging run\n",
      "\n",
      "Done.\n",
      "Outputs written to, /Users/petalc01/Connectomics C.elegans EM cutout/outputs\n",
      "Figures, /Users/petalc01/Connectomics C.elegans EM cutout/outputs/figures\n",
      "Tables, /Users/petalc01/Connectomics C.elegans EM cutout/outputs/tables\n",
      "Aligned volume saved as, outputs/data/em_aligned_float32_zyx.npy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage.filters import gaussian, sobel\n",
    "from skimage.morphology import remove_small_objects, binary_opening, disk\n",
    "from skimage.segmentation import watershed, find_boundaries\n",
    "from skimage.feature import peak_local_max, blob_log\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.transform import warp\n",
    "from skimage.transform._warps import SimilarityTransform\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "try:\n",
    "    from intern import array as boss_array\n",
    "except Exception:\n",
    "    boss_array = None\n",
    "\n",
    "\n",
    "\n",
    "# Config\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    dataset_uri: str\n",
    "    outdir: Path\n",
    "    cutout_xyz: Tuple[int, int, int]  # (X, Y, Z)\n",
    "    seed: int = 7\n",
    "    simulate_misalignment: bool = True\n",
    "    save_minimal: bool = False\n",
    "    aligned_dtype: str = \"float32\"\n",
    "\n",
    "\n",
    "# I/O helpers\n",
    "\n",
    "def ensure_dirs(outdir: Path) -> Dict[str, Path]:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    paths = {\n",
    "        \"root\": outdir,\n",
    "        \"data\": outdir / \"data\",\n",
    "        \"figures\": outdir / \"figures\",\n",
    "        \"tables\": outdir / \"tables\",\n",
    "        \"cache\": outdir / \"cache\",\n",
    "        \"runs\": outdir / \"runs\",\n",
    "    }\n",
    "    for p in paths.values():\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def init_db(db_path: Path) -> None:\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS runs (\n",
    "            run_id TEXT PRIMARY KEY,\n",
    "            dataset_uri TEXT,\n",
    "            cutout_x INTEGER,\n",
    "            cutout_y INTEGER,\n",
    "            cutout_z INTEGER,\n",
    "            created_at TEXT,\n",
    "            meta_json TEXT\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def log_run(db_path: Path, run_id: str, cfg: RunConfig, meta: Dict) -> None:\n",
    "    con = sqlite3.connect(db_path)\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO runs(run_id, dataset_uri, cutout_x, cutout_y, cutout_z, created_at, meta_json)\n",
    "        VALUES (?, ?, ?, ?, ?, datetime('now'), ?);\n",
    "        \"\"\",\n",
    "        (\n",
    "            run_id,\n",
    "            cfg.dataset_uri,\n",
    "            cfg.cutout_xyz[0],\n",
    "            cfg.cutout_xyz[1],\n",
    "            cfg.cutout_xyz[2],\n",
    "            json.dumps(meta, indent=2),\n",
    "        ),\n",
    "    )\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def save_json(path: Path, obj: Dict) -> None:\n",
    "    path.write_text(json.dumps(obj, indent=2))\n",
    "\n",
    "\n",
    "def save_figure(fig, path_png: Path, path_pdf: Path, dpi: int = 300) -> None:\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "    fig.savefig(path_pdf, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def sizeof_gb(n_bytes: int) -> float:\n",
    "    return float(n_bytes) / (1024.0 ** 3)\n",
    "\n",
    "\n",
    "def estimate_volume_bytes(xyz: Tuple[int, int, int], dtype: str) -> int:\n",
    "    x, y, z = xyz\n",
    "    n = x * y * z\n",
    "    b = np.dtype(dtype).itemsize\n",
    "    # volume arrays are stored as (Z,Y,X), size is the same\n",
    "    return n * b\n",
    "\n",
    "\n",
    "# 1) Download data (small cutout)\n",
    "\n",
    "def download_bossdb_cutout(dataset_uri: str, cutout_xyz: Tuple[int, int, int], seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Download a small cutout from a BossDB dataset using intern.\n",
    "\n",
    "    Returns:\n",
    "        vol_zyx: uint8 volume shaped (Z, Y, X)\n",
    "    \"\"\"\n",
    "    if boss_array is None:\n",
    "        raise RuntimeError(\"Could not import 'intern'. Install it with: pip install intern\")\n",
    "\n",
    "    em = boss_array(dataset_uri)\n",
    "\n",
    "    # intern uses Z,Y,X indexing, em.shape is (Z,Y,X)\n",
    "    zyx_shape = em.shape\n",
    "    Z, Y, X = int(zyx_shape[0]), int(zyx_shape[1]), int(zyx_shape[2])\n",
    "\n",
    "    cx, cy, cz = X // 2, Y // 2, Z // 2\n",
    "    sx, sy, sz = cutout_xyz\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # pick a random center near the middle (avoid empty edges)\n",
    "    jitter_x = int(rng.integers(-X * 0.05, X * 0.05))\n",
    "    jitter_y = int(rng.integers(-Y * 0.05, Y * 0.05))\n",
    "    jitter_z = int(rng.integers(-Z * 0.05, Z * 0.05))\n",
    "\n",
    "    x0 = max(0, min(X - sx, cx - sx // 2 + jitter_x))\n",
    "    y0 = max(0, min(Y - sy, cy - sy // 2 + jitter_y))\n",
    "    z0 = max(0, min(Z - sz, cz - sz // 2 + jitter_z))\n",
    "\n",
    "    x1, y1, z1 = x0 + sx, y0 + sy, z0 + sz\n",
    "\n",
    "    vol = em[z0:z1, y0:y1, x0:x1]\n",
    "\n",
    "    if vol.dtype != np.uint8:\n",
    "        vol = exposure.rescale_intensity(vol, out_range=(0, 255)).astype(np.uint8)\n",
    "\n",
    "    return vol\n",
    "\n",
    "\n",
    "# 2) Preprocess (denoise + normalize)\n",
    "\n",
    "def preprocess_volume(vol_zyx_u8: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Per-slice preprocessing for EM:\n",
    "    - light gaussian denoise\n",
    "    - contrast normalization via CLAHE\n",
    "    Returns float32 volume in [0,1].\n",
    "    \"\"\"\n",
    "    out = np.empty_like(vol_zyx_u8, dtype=np.float32)\n",
    "    for z in range(vol_zyx_u8.shape[0]):\n",
    "        img = vol_zyx_u8[z].astype(np.float32) / 255.0\n",
    "        img = gaussian(img, sigma=0.7, preserve_range=True)\n",
    "        img = exposure.equalize_adapthist(img, clip_limit=0.01)\n",
    "        out[z] = img.astype(np.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "# 3) Align slices (rigid translation)\n",
    "\n",
    "def simulate_misalignment(vol: np.ndarray, seed: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply small random translations to each slice to simulate misalignment.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Z = vol.shape[0]\n",
    "    out = np.empty_like(vol)\n",
    "    for z in range(Z):\n",
    "        dy = float(rng.normal(0, 1.0))\n",
    "        dx = float(rng.normal(0, 1.0))\n",
    "        tform = SimilarityTransform(translation=(dx, dy))\n",
    "        out[z] = warp(vol[z], tform.inverse, preserve_range=True, mode=\"edge\").astype(vol.dtype)\n",
    "    return out\n",
    "\n",
    "\n",
    "def align_slices_translation(vol: np.ndarray) -> Tuple[np.ndarray, List[Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Align each slice to the previous using phase cross-correlation (translation only).\n",
    "    Returns aligned volume and list of (dy, dx) shifts.\n",
    "    \"\"\"\n",
    "    Z = vol.shape[0]\n",
    "    aligned = np.empty_like(vol)\n",
    "    aligned[0] = vol[0]\n",
    "    shifts = [(0.0, 0.0)]\n",
    "\n",
    "    ref = vol[0]\n",
    "    for z in range(1, Z):\n",
    "        shift, _, _ = phase_cross_correlation(ref, vol[z], upsample_factor=10)\n",
    "        dy, dx = float(shift[0]), float(shift[1])\n",
    "        tform = SimilarityTransform(translation=(-dx, -dy))\n",
    "        aligned[z] = warp(vol[z], tform.inverse, preserve_range=True, mode=\"edge\").astype(vol.dtype)\n",
    "        shifts.append((dy, dx))\n",
    "        ref = aligned[z]\n",
    "    return aligned, shifts\n",
    "\n",
    "\n",
    "# 4) Segmentation (watershed baseline + 3D stitching)\n",
    "\n",
    "def relabel_compact(lbl: np.ndarray) -> np.ndarray:\n",
    "    ids = np.unique(lbl)\n",
    "    ids = ids[ids != 0]\n",
    "    mapping = {old: new for new, old in enumerate(ids, start=1)}\n",
    "    out = lbl.copy()\n",
    "    mask = out > 0\n",
    "    if mask.any():\n",
    "        out[mask] = np.vectorize(mapping.get)(out[mask])\n",
    "    return out.astype(np.int32)\n",
    "\n",
    "\n",
    "def stitch_labels_by_overlap(labels_zyx: np.ndarray, min_overlap: int = 200) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Stitch 2D labels into consistent 3D IDs by overlapping area across adjacent slices.\n",
    "\n",
    "    Strategy:\n",
    "    - make all slice labels unique by offsetting\n",
    "    - merge IDs across adjacent slices by large overlaps\n",
    "    \"\"\"\n",
    "    Z = labels_zyx.shape[0]\n",
    "    out = labels_zyx.copy()\n",
    "\n",
    "    current_max = 0\n",
    "    for z in range(Z):\n",
    "        sl = out[z]\n",
    "        sl_nonzero = sl > 0\n",
    "        if sl_nonzero.any():\n",
    "            sl = sl.copy()\n",
    "            sl[sl_nonzero] += current_max\n",
    "            out[z] = sl\n",
    "            current_max = int(out[z].max())\n",
    "\n",
    "        parent = {}\n",
    "\n",
    "    def find(a: int) -> int:\n",
    "        # Find root\n",
    "        root = a\n",
    "        while parent.get(root, root) != root:\n",
    "            root = parent.get(root, root)\n",
    "\n",
    "        # Path compression (safe)\n",
    "        while parent.get(a, a) != a:\n",
    "            p = parent.get(a, a)\n",
    "            parent[a] = root\n",
    "            a = p\n",
    "\n",
    "        return root\n",
    "\n",
    "    def union(a: int, b: int) -> None:\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            parent[rb] = ra\n",
    "\n",
    "    for z in tqdm(range(Z - 1), desc=\"Stitching 3D IDs\"):\n",
    "        a = out[z]\n",
    "        b = out[z + 1]\n",
    "        mask = a > 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        pairs = np.stack([a[mask], b[mask]], axis=1)\n",
    "        uniq, counts = np.unique(pairs, axis=0, return_counts=True)\n",
    "        for (aid, bid), cnt in zip(uniq, counts):\n",
    "            aid = int(aid)\n",
    "            bid = int(bid)\n",
    "            if bid == 0:\n",
    "                continue\n",
    "            if cnt >= min_overlap:\n",
    "                union(aid, bid)\n",
    "\n",
    "    flat = out.reshape(-1)\n",
    "    nonzero = flat > 0\n",
    "    ids = flat[nonzero]\n",
    "    if ids.size > 0:\n",
    "        flat[nonzero] = np.array([find(int(x)) for x in ids], dtype=np.int32)\n",
    "\n",
    "    out = flat.reshape(out.shape)\n",
    "    out = relabel_compact(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def segment_neurites_watershed(vol: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Baseline neurite segmentation (proof-of-concept):\n",
    "    - foreground proxy by quantile thresholding\n",
    "    - watershed from distance seeds\n",
    "    - slice-wise labels stitched to 3D IDs by overlap\n",
    "    \"\"\"\n",
    "    Z, Y, X = vol.shape\n",
    "    labels = np.zeros((Z, Y, X), dtype=np.int32)\n",
    "\n",
    "    for z in tqdm(range(Z), desc=\"Segmenting slices\"):\n",
    "        img = vol[z]\n",
    "\n",
    "        thr = np.quantile(img, 0.55)\n",
    "        fg = img > thr\n",
    "        fg = binary_opening(fg, disk(1))\n",
    "        fg = remove_small_objects(fg, min_size=128)\n",
    "\n",
    "        if fg.sum() < 256:\n",
    "            continue\n",
    "\n",
    "        dist = ndi.distance_transform_edt(fg)\n",
    "        coords = peak_local_max(dist, min_distance=8, labels=fg, exclude_border=False)\n",
    "\n",
    "        markers = np.zeros_like(img, dtype=np.int32)\n",
    "        for i, (r, c) in enumerate(coords, start=1):\n",
    "            markers[r, c] = i\n",
    "        markers = ndi.label(markers > 0)[0]\n",
    "        if markers.max() == 0:\n",
    "            continue\n",
    "\n",
    "        elevation = sobel(img)\n",
    "        seg = watershed(elevation, markers=markers, mask=fg)\n",
    "        labels[z] = seg.astype(np.int32)\n",
    "\n",
    "    labels = stitch_labels_by_overlap(labels, min_overlap=200)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# 5) Synapse candidate detection (heuristic)\n",
    "\n",
    "def detect_synapse_candidates(vol: np.ndarray, labels: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Proof-of-concept synapse candidate detection:\n",
    "    - boundaries between segments\n",
    "    - dark blob-like features near boundaries (blob_log on inverted intensity)\n",
    "    - assign candidate to (pre_id, post_id) based on local neighborhood labels\n",
    "    Direction is heuristic, used only to construct a directed graph.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    Z = vol.shape[0]\n",
    "\n",
    "    for z in tqdm(range(Z), desc=\"Detecting synapse candidates\"):\n",
    "        img = vol[z]\n",
    "        lbl = labels[z]\n",
    "\n",
    "        if lbl.max() < 2:\n",
    "            continue\n",
    "\n",
    "        bnd = find_boundaries(lbl, mode=\"outer\")\n",
    "        if bnd.sum() < 64:\n",
    "            continue\n",
    "\n",
    "        img_n = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "        inv = 1.0 - img_n\n",
    "\n",
    "        blobs = blob_log(inv, min_sigma=1, max_sigma=4, num_sigma=6, threshold=0.08)\n",
    "\n",
    "        for (y, x, s) in blobs:\n",
    "            y = int(round(y))\n",
    "            x = int(round(x))\n",
    "            if y < 2 or x < 2 or y >= img.shape[0] - 2 or x >= img.shape[1] - 2:\n",
    "                continue\n",
    "\n",
    "            y0, y1 = max(0, y - 3), min(img.shape[0], y + 4)\n",
    "            x0, x1 = max(0, x - 3), min(img.shape[1], x + 4)\n",
    "            if bnd[y0:y1, x0:x1].sum() == 0:\n",
    "                continue\n",
    "\n",
    "            neigh = lbl[y0:y1, x0:x1]\n",
    "            ids = neigh[neigh > 0]\n",
    "            if ids.size < 10:\n",
    "                continue\n",
    "            uniq, cnt = np.unique(ids, return_counts=True)\n",
    "            if uniq.size < 2:\n",
    "                continue\n",
    "\n",
    "            order = np.argsort(-cnt)\n",
    "            a = int(uniq[order[0]])\n",
    "            b = int(uniq[order[1]])\n",
    "\n",
    "            win = inv[y0:y1, x0:x1]\n",
    "            mean_a = float(win[neigh == a].mean()) if (neigh == a).any() else 0.0\n",
    "            mean_b = float(win[neigh == b].mean()) if (neigh == b).any() else 0.0\n",
    "\n",
    "            if mean_a >= mean_b:\n",
    "                pre_id, post_id, score = a, b, mean_a\n",
    "            else:\n",
    "                pre_id, post_id, score = b, a, mean_b\n",
    "\n",
    "            rows.append({\"z\": z, \"y\": y, \"x\": x, \"pre_id\": pre_id, \"post_id\": post_id, \"score\": score})\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"z\", \"y\", \"x\", \"pre_id\", \"post_id\", \"score\"])\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # de-duplicate close detections\n",
    "    \n",
    "    df[\"key\"] = (\n",
    "        df[\"z\"].astype(int).astype(str) + \"_\" +\n",
    "        (df[\"y\"].astype(int) // 4).astype(str) + \"_\" +\n",
    "        (df[\"x\"].astype(int) // 4).astype(str) + \"_\" +\n",
    "        df[\"pre_id\"].astype(int).astype(str) + \"_\" +\n",
    "        df[\"post_id\"].astype(int).astype(str)\n",
    "    )\n",
    "    df = df.drop_duplicates(\"key\", keep=\"first\").drop(columns=[\"key\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 6) Graph\n",
    "\n",
    "def build_graph(syn_df: pd.DataFrame, min_score: float = 0.10) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    if syn_df.empty:\n",
    "        return G\n",
    "\n",
    "    use = syn_df[syn_df[\"score\"] >= min_score].copy()\n",
    "    for _, r in use.iterrows():\n",
    "        a = int(r[\"pre_id\"])\n",
    "        b = int(r[\"post_id\"])\n",
    "        if a == b:\n",
    "            continue\n",
    "        if G.has_edge(a, b):\n",
    "            G[a][b][\"weight\"] += 1.0\n",
    "        else:\n",
    "            G.add_edge(a, b, weight=1.0)\n",
    "    return G\n",
    "\n",
    "\n",
    "def graph_stats(G: nx.DiGraph) -> pd.DataFrame:\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return pd.DataFrame(columns=[\"node\", \"in_degree\", \"out_degree\", \"pagerank\"])\n",
    "\n",
    "    indeg = dict(G.in_degree(weight=\"weight\"))\n",
    "    outdeg = dict(G.out_degree(weight=\"weight\"))\n",
    "    pr = nx.pagerank(G, weight=\"weight\") if G.number_of_edges() > 0 else {n: 0.0 for n in G.nodes()}\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"node\": list(G.nodes()),\n",
    "            \"in_degree\": [indeg.get(n, 0.0) for n in G.nodes()],\n",
    "            \"out_degree\": [outdeg.get(n, 0.0) for n in G.nodes()],\n",
    "            \"pagerank\": [pr.get(n, 0.0) for n in G.nodes()],\n",
    "        }\n",
    "    ).sort_values(\"pagerank\", ascending=False)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 7) Figures\n",
    "\n",
    "def plot_overview(vol_raw_u8: np.ndarray,\n",
    "                  vol_proc_f: np.ndarray,\n",
    "                  vol_aligned: np.ndarray,\n",
    "                  labels: np.ndarray,\n",
    "                  syn_df: pd.DataFrame,\n",
    "                  out_fig_dir: Path) -> None:\n",
    "    Z = vol_raw_u8.shape[0]\n",
    "    mid = Z // 2\n",
    "\n",
    "    raw = vol_raw_u8[mid]\n",
    "    proc = vol_proc_f[mid]\n",
    "    ali = vol_aligned[mid]\n",
    "    lbl = labels[mid]\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    ax[0, 0].imshow(raw, cmap=\"gray\")\n",
    "    ax[0, 0].set_title(\"Raw EM slice (mid-Z)\")\n",
    "    ax[0, 0].axis(\"off\")\n",
    "\n",
    "    ax[0, 1].imshow(proc, cmap=\"gray\")\n",
    "    ax[0, 1].set_title(\"Preprocessed slice (CLAHE + denoise)\")\n",
    "    ax[0, 1].axis(\"off\")\n",
    "\n",
    "    ax[1, 0].imshow(ali, cmap=\"gray\")\n",
    "    ax[1, 0].set_title(\"Aligned slice (translation)\")\n",
    "    ax[1, 0].axis(\"off\")\n",
    "\n",
    "    ax[1, 1].imshow(lbl, cmap=\"nipy_spectral\")\n",
    "    ax[1, 1].set_title(f\"Neurite segments (IDs: {int(lbl.max())})\")\n",
    "    ax[1, 1].axis(\"off\")\n",
    "\n",
    "    save_figure(\n",
    "        fig,\n",
    "        out_fig_dir / \"Figure1_EM_preprocess_align_segment.png\",\n",
    "        out_fig_dir / \"Figure1_EM_preprocess_align_segment.pdf\",\n",
    "    )\n",
    "\n",
    "    fig2, ax2 = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax2.imshow(ali, cmap=\"gray\")\n",
    "    if not syn_df.empty:\n",
    "        show = syn_df[syn_df[\"z\"] == mid]\n",
    "        ax2.scatter(show[\"x\"], show[\"y\"], s=10)\n",
    "        ax2.set_title(f\"Synapse candidates (mid-Z), n={len(show)}\")\n",
    "    else:\n",
    "        ax2.set_title(\"Synapse candidates (none found in mid-Z)\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    save_figure(\n",
    "        fig2,\n",
    "        out_fig_dir / \"Figure2_synapse_candidates_overlay.png\",\n",
    "        out_fig_dir / \"Figure2_synapse_candidates_overlay.pdf\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_graph(G: nx.DiGraph, stats_df: pd.DataFrame, out_fig_dir: Path) -> None:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if G.number_of_nodes() == 0:\n",
    "        ax.set_title(\"Connectivity graph (empty)\")\n",
    "        save_figure(fig, out_fig_dir / \"Figure3_graph.png\", out_fig_dir / \"Figure3_graph.pdf\")\n",
    "        return\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=7, k=0.8 / np.sqrt(max(G.number_of_nodes(), 1)))\n",
    "    weights = np.array([G[u][v][\"weight\"] for u, v in G.edges()])\n",
    "    widths = 0.5 + 2.0 * (weights / (weights.max() + 1e-6))\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, width=widths, alpha=0.6, arrows=True, arrowsize=10)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=200, alpha=0.9)\n",
    "\n",
    "    top = stats_df.head(10)[\"node\"].tolist() if not stats_df.empty else []\n",
    "    labels = {n: str(n) for n in top}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Connectivity graph, nodes={G.number_of_nodes()}, edges={G.number_of_edges()}\")\n",
    "    save_figure(fig, out_fig_dir / \"Figure3_graph.png\", out_fig_dir / \"Figure3_graph.pdf\")\n",
    "\n",
    "\n",
    "def plot_alignment_shifts(shifts: List[Tuple[float, float]], out_fig_dir: Path) -> None:\n",
    "    if not shifts:\n",
    "        return\n",
    "\n",
    "    dy = [s[0] for s in shifts]\n",
    "    dx = [s[1] for s in shifts]\n",
    "    z = np.arange(len(shifts))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    ax.plot(z, dy, label=\"dy\")\n",
    "    ax.plot(z, dx, label=\"dx\")\n",
    "    ax.set_xlabel(\"Slice (Z)\")\n",
    "    ax.set_ylabel(\"Estimated shift (pixels)\")\n",
    "    ax.set_title(\"Estimated per-slice alignment shifts\")\n",
    "    ax.legend()\n",
    "\n",
    "    save_figure(fig, out_fig_dir / \"Figure4_alignment_shifts.png\", out_fig_dir / \"Figure4_alignment_shifts.pdf\")\n",
    "\n",
    "\n",
    "# CLI\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"outputs\", help=\"Output directory\")\n",
    "    p.add_argument(\"--dataset\", type=str, default=\"bossdb://mulcahy2022/1h_L1/em\", help=\"BossDB dataset URI\")\n",
    "    p.add_argument(\"--preset\", type=str, default=\"none\",\n",
    "                   choices=[\"none\", \"safe8gb\", \"medium\"],\n",
    "                   help=\"Convenience preset for cutout size\")\n",
    "    p.add_argument(\"--cutout\", type=int, nargs=3, default=[256, 256, 64], metavar=(\"X\", \"Y\", \"Z\"),\n",
    "                   help=\"Cutout size in voxels (X Y Z), ignored if preset != none\")\n",
    "    p.add_argument(\"--save_minimal\", action=\"store_true\",\n",
    "                   help=\"Save only aligned volume, labels, tables, figures, keeps disk usage low\")\n",
    "    p.add_argument(\"--aligned_dtype\", type=str, default=\"float32\", choices=[\"float16\", \"float32\"],\n",
    "                   help=\"Storage dtype for aligned volume, float16 is smaller\")\n",
    "    p.add_argument(\"--no-sim-misalignment\", action=\"store_true\",\n",
    "                   help=\"Disable misalignment simulation step\")\n",
    "    p.add_argument(\"--seed\", type=int, default=7, help=\"Random seed\")\n",
    "\n",
    "    # Jupyter passes extra args like \"-f <kernel.json>\", ignore them\n",
    "    args, _ = p.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def preset_cutout(preset: str) -> Tuple[int, int, int]:\n",
    "    if preset == \"safe8gb\":\n",
    "        return (192, 192, 48)\n",
    "    if preset == \"medium\":\n",
    "        return (256, 256, 48)\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cutout_xyz = tuple(int(x) for x in args.cutout)\n",
    "    if args.preset != \"none\":\n",
    "        cutout_xyz = preset_cutout(args.preset)\n",
    "\n",
    "    cfg = RunConfig(\n",
    "        dataset_uri=args.dataset,\n",
    "        outdir=Path(args.outdir),\n",
    "        cutout_xyz=cutout_xyz,\n",
    "        seed=int(args.seed),\n",
    "        simulate_misalignment=(not args.no_sim_misalignment),\n",
    "        save_minimal=bool(args.save_minimal),\n",
    "        aligned_dtype=str(args.aligned_dtype),\n",
    "    )\n",
    "\n",
    "    paths = ensure_dirs(cfg.outdir)\n",
    "    db_path = paths[\"runs\"] / \"pipeline_runs.sqlite\"\n",
    "    init_db(db_path)\n",
    "\n",
    "    run_id = f\"run_{cfg.seed}_{cfg.cutout_xyz[0]}x{cfg.cutout_xyz[1]}x{cfg.cutout_xyz[2]}_{cfg.aligned_dtype}\"\n",
    "    meta = {\n",
    "        \"run_id\": run_id,\n",
    "        \"dataset_uri\": cfg.dataset_uri,\n",
    "        \"cutout_xyz\": cfg.cutout_xyz,\n",
    "        \"simulate_misalignment\": cfg.simulate_misalignment,\n",
    "        \"save_minimal\": cfg.save_minimal,\n",
    "        \"aligned_dtype\": cfg.aligned_dtype,\n",
    "        \"env_OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\", \"\"),\n",
    "        \"env_MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\", \"\"),\n",
    "    }\n",
    "    save_json(paths[\"runs\"] / f\"{run_id}_config.json\", meta)\n",
    "\n",
    "    # sanity estimates\n",
    "    raw_bytes = estimate_volume_bytes(cfg.cutout_xyz, \"uint8\")\n",
    "    pre_bytes = estimate_volume_bytes(cfg.cutout_xyz, \"float32\")\n",
    "    ali_bytes = estimate_volume_bytes(cfg.cutout_xyz, cfg.aligned_dtype)\n",
    "    print(\"\\nPlanned run settings\")\n",
    "    print(f\"Dataset URI, {cfg.dataset_uri}\")\n",
    "    print(f\"Cutout (X,Y,Z), {cfg.cutout_xyz}\")\n",
    "    print(f\"Aligned dtype, {cfg.aligned_dtype}\")\n",
    "    print(f\"save_minimal, {cfg.save_minimal}\")\n",
    "    print(\"\\nApproximate single-volume sizes\")\n",
    "    print(f\"Raw uint8 volume, {sizeof_gb(raw_bytes):.3f} GB\")\n",
    "    print(f\"Preprocessed float32 volume, {sizeof_gb(pre_bytes):.3f} GB\")\n",
    "    print(f\"Aligned {cfg.aligned_dtype} volume, {sizeof_gb(ali_bytes):.3f} GB\\n\")\n",
    "\n",
    "    # 1) Download\n",
    "    print(f\"[1/8] Downloading cutout from: {cfg.dataset_uri}\")\n",
    "    vol_u8 = download_bossdb_cutout(cfg.dataset_uri, cfg.cutout_xyz, cfg.seed)\n",
    "    if not cfg.save_minimal:\n",
    "        np.save(paths[\"data\"] / \"em_cutout_u8_zyx.npy\", vol_u8)\n",
    "\n",
    "    # 2) Preprocess\n",
    "    print(\"[2/8] Preprocessing (denoise + CLAHE)\")\n",
    "    vol_f = preprocess_volume(vol_u8)\n",
    "    if not cfg.save_minimal:\n",
    "        np.save(paths[\"data\"] / \"em_preprocessed_f32_zyx.npy\", vol_f.astype(np.float32))\n",
    "\n",
    "    # 3) Align\n",
    "    print(\"[3/8] Alignment\")\n",
    "    vol_for_align = vol_f.copy()\n",
    "    if cfg.simulate_misalignment:\n",
    "        print(\"Simulating misalignment, then correcting it\")\n",
    "        vol_for_align = simulate_misalignment(vol_for_align, cfg.seed)\n",
    "\n",
    "    aligned, shifts = align_slices_translation(vol_for_align)\n",
    "\n",
    "    # store aligned in chosen dtype\n",
    "    if cfg.aligned_dtype == \"float16\":\n",
    "        aligned_store = aligned.astype(np.float16)\n",
    "    else:\n",
    "        aligned_store = aligned.astype(np.float32)\n",
    "\n",
    "    np.save(paths[\"data\"] / f\"em_aligned_{cfg.aligned_dtype}_zyx.npy\", aligned_store)\n",
    "    save_json(paths[\"tables\"] / \"alignment_shifts.json\", {\"shifts\": shifts})\n",
    "\n",
    "    # 4) Segmentation\n",
    "    print(\"[4/8] Segmentation (watershed baseline + 3D stitching)\")\n",
    "    # use float32 for computation to avoid numerical issues, even if stored float16\n",
    "    aligned_f32 = aligned.astype(np.float32, copy=False)\n",
    "    labels = segment_neurites_watershed(aligned_f32)\n",
    "    np.save(paths[\"data\"] / \"neurite_labels_i32_zyx.npy\", labels)\n",
    "\n",
    "    # 5) Synapse candidates\n",
    "    print(\"[5/8] Synapse candidate detection (heuristic)\")\n",
    "    syn_df = detect_synapse_candidates(aligned_f32, labels)\n",
    "    syn_df.to_csv(paths[\"tables\"] / \"synapse_candidates.csv\", index=False)\n",
    "\n",
    "    # 6) Graph\n",
    "    print(\"[6/8] Build wiring graph\")\n",
    "    G = build_graph(syn_df, min_score=0.10)\n",
    "    stats_df = graph_stats(G)\n",
    "    stats_df.to_csv(paths[\"tables\"] / \"graph_node_stats.csv\", index=False)\n",
    "\n",
    "    # 7) Figures\n",
    "    print(\"[7/8] Saving figures\")\n",
    "    # For figures, use the float32 aligned slice for display quality\n",
    "    plot_overview(vol_u8, vol_f, aligned_f32, labels, syn_df, paths[\"figures\"])\n",
    "    plot_graph(G, stats_df, paths[\"figures\"])\n",
    "    plot_alignment_shifts(shifts, paths[\"figures\"])\n",
    "\n",
    "    # 8) Log run\n",
    "    print(\"[8/8] Logging run\")\n",
    "    log_run(db_path, run_id, cfg, meta)\n",
    "\n",
    "    # If save_minimal, free memory aggressively\n",
    "    if cfg.save_minimal:\n",
    "        del vol_u8, vol_f, vol_for_align, aligned, aligned_store, aligned_f32\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Outputs written to, {cfg.outdir.resolve()}\")\n",
    "    print(f\"Figures, {paths['figures'].resolve()}\")\n",
    "    print(f\"Tables, {paths['tables'].resolve()}\")\n",
    "    print(f\"Aligned volume saved as, {paths['data'] / f'em_aligned_{cfg.aligned_dtype}_zyx.npy'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd895b-e48f-48ad-85cb-7a8e05868c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
